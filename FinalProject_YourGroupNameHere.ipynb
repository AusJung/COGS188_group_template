{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating AI Algorithms on PacMan\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Anvita Suresh\n",
    "- Austin Jung\n",
    "- Aarohi Zade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of our project is to develop an AI agent that is able to beat the game of PacMan autonomously. The agent will be responsible for navigating through a provided maze with the goal of optimizing the amount of pellets collected while avoiding the ghosts. The data that we would use would be the environment of the game including the layout of the maze, pellets that represent the rewards of the game, and more. More specifically, we will be using OpenAI Gym to provide a simulated training environment for our project. We will employ AI algorithms to train the AI agent to make decisions based on different game states. The chosen algorithm should minimize the time spent playing the game and avoid terminating the game (avoiding colliding with ghosts). We will measure the success of the AI agent during the game based on the following metrics: speed of completion, wins and losses, and number of pellets collected if lost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Pac-Man is a retro video game where the goal is to navigate through a maze-like environment, while collecting pellets for points and evading the enemy ghosts. Since the creation of the game, many people have tried their hardest to collect all of the pellets the fastest, and in recent years, this retro game has become a playground to test different AI algorithms.\n",
    "\n",
    "Various AI algorithms have been tested on the original retro arcade, and some researchers [<sup>[1]</sup>](#swetha). have determined A* search is the optimal solution. Although the article[^1] explores many of the different algorithms, it does not delve into the experimental setups or real-time feasibility. Rather, it reads as more like explaining the optimal solutions to navigating the map.\n",
    "\n",
    "Another group of researchers[<sup>[2]</sup>](#pepels) explored the effectiveness of the Monte Carlo Search Tree when controlling Ms. Pac-Man, particularly when working in real time. The article[<sup>[2]</sup>](#pepels) specifically focused on real-time strategies, specifically using < 40 ms decision making to simulate a real game, and determined that using this algorithm created a strong competitive agent in gathering points.\n",
    "\n",
    "Using the OpenAi Gym[<sup>[3]</sup>](#gupta), this blogger showed how to set up an environment to instruct the agent, using a random search. Although Gupta’s work primarily focuses on the random search rather than AI, it serves as a valuable resource in setting up the environment to work on implementing our own algorithms.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Our team’s objective is to implement an AI algorithm taught in class, and tune it to the retro arcade game Ms. Pac-Man. We want the algorithm to control the agent Ms. Pac-Man, which will evade the ghosts and collect the pellets scattered around the map.\n",
    "\n",
    "- Initial State: The map with all of the pellets in position, Ms. Pac-Man at the starting position (centered horizontally and slightly shifted down vertically), and all of the ghosts in the enclosure.\n",
    "- Players: Only 1 player, which is Ms. Pac-Man.\n",
    "- Actions: At every cell, the possible legal moves are going left, right, up, down, and no action. Even when there is a wall at the potential move, the action of Ms. Pac-Man will still turn towards that direction.\n",
    "- Result: The new state of the map, which typically consists of Ms. Pac-Man facing a new position and/or moving along the directed path, along with the removal of pellets from the new cell Ms. Pac-Man is on. Finally, this also results in any changes the random actions of the ghosts have.\n",
    "- Terminal Test: The game ends (reaches terminal state) when either all the pellets have been collected or when the ghosts reach pacman.\n",
    "- Utility: The final outcome will be the total number of pellets collected. So the lower the score, the less pellets that were collected before death, with the maximum score being that all pellets are collected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In order to achieve our goal of maximizing our AI agent’s performance in the game of Pacman, we intend to explore a few search algorithms that are discussed in class, namely, the A*, Monte Carlo Search Tree, and Greedy algorithms. Upon implementing each of these algorithms, we will evaluate based on a few metrics such as speed of completion and win/loss ratio to find the optimal policy for the game, which we will establish as our solution. The A algorithm is a node search procedure that can be useful to minimize a particular cost based on a certain heuristic of success, which could be helpful in optimizing the agent’s decisions. Examples of heuristics we might employ include maximizing distance to the nearest ghost or minimizing the distance to the nearest power-up. Monte Carlo Search Tree (MCST) is a heuristic search procedure that is particularly useful when the search space is more complex. We can use MCTS for our Pacman agent to predict the potential outcomes of moving in different directions, considering the actions of ghosts whose positions at any given time are not fully known. By simulating multiple possible scenarios and evaluating their outcomes, Pacman can make informed decisions to maximize its chances of survival, leading to a successful policy. Lastly, the Greedy search algorithm can help us evaluate how Pacman can approaches the closest dot in a greedy manner. It is a sub-optimal algorithm that will deterministically choose one dot based on its current position. Based on these three algorithms and how they perform on the given environment, we can select an algorithm with the most effective performance based on the evaluation metrics that we will discuss below. Furthermore, we will implement the Minimax algorithm and alpha beta pruning in addition to each of these AI algorithms. We are doing this in order to choose moves that maximize Pac-Man's minimum possible score (or minimize the maximum possible score of the opponent, which are the ghosts in this case). It helps Pacman choose actions to maximize score while also considering the moves of the ghosts that may potentially limit the score. The alpha/beta pruning is an optimization technique that is used to get rid of possible future states that are irrelevant because they would not lead to more beneficial outcomes than already explored alternatives; in other words, it reduces the number of nodes evaluated by the Minimax algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "In order to evaluate the performance of our AI agent in playing PacMan, we will use several key metrics to measure and quantify its effectiveness. Firstly, we will consider the speed of completion. A faster completion time indicates that the agent has been trained properly and is m}ore efficient. We will also track the agent's win-loss ratio, which reflects its ability to successfully complete the game by avoiding termination (colliding with ghosts). If the agent is able to demonstrate a high win to loss ratio, we can prove the agent's skill in evading ghosts and completing the game. Lastly, we will monitor the number of pellets collected if the agent loses. We want to evaluate the agent's performance in maximizing its rewards even if it is unable to complete the game as desired. By evaluating these metrics comprehensively, we can assess the overall proficiency of our AI agent in mastering the game of PacMan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Information About our Project\n",
    "We tried working with the observation/state space of the Open AI gym MsPacman, but from what we could figure out, the state space was given to us based off the rgb values of each pixel. The numpy array given to us gives us rgb values for each pixel rather than the grid, with values for each coordinate. We tried to look for a way to convert the rgb values to grid values with pellet, ghost, and pacman information, but we were unable to find a way to do so. Therefore, some of our code doesn’t run and our code below is mostly based off what we imagine would be correct if we were to have state spaces that were workable (we tried for days to no avail).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's evaluate the three search algorithms based on the evaluation metrics that we discussed above.\n",
    "### Greedy Search\n",
    "\n",
    "Greedy search is an AI search algorithm that decides on the locally optimal choice that seems most optimal at the current moment. It will choose the option that is able to maximize the immediate reward given the evaluation function without considering the long-term consequences of doing so. It can lead to suboptimal solutions because the algorithm proceeds iteratively, selecting the next step and not engaging in backtracking. As a result, Greedy search is computationally efficient because it can focus on immediate decisions rather than exhaustive search but it will not always converge on optimal solution, especially if early decisions made by the agent do not contribute to the best solution in that environemnt. \n",
    "\n",
    "First, let's install the necessary libraries as well as implement the Minimax algorithm with alpha/beta pruning. This will help us later determine which combination of algorithm and search strategy optimizes Pac-Man's performance in terms of efficiency and rate of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: gym[atari] in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym[atari]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym[atari]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym[atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from ale-py~=0.8.0->gym[atari]) (6.4.0)\n",
      "Requirement already satisfied: numpy in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: gym[accept-rom-license] in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym[accept-rom-license]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym[accept-rom-license]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from gym[accept-rom-license]) (0.0.8)\n",
      "Requirement already satisfied: autorom~=0.4.2 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: click in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (8.1.7)\n",
      "Requirement already satisfied: requests in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (4.66.4)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2024.6.2)\n",
      "Requirement already satisfied: imageio in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (2.34.1)\n",
      "Requirement already satisfied: numpy in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from imageio) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages (from imageio) (10.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install \"gym[atari]\"\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install \"gym[accept-rom-license]\"\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/austin/anaconda3/envs/FinalProjCogs188/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "# initialize the Ms. Pac-Man environment\n",
    "env = gym.make('MsPacman-v4', render_mode='rgb_array')\n",
    "state, _ = env.reset()\n",
    "\n",
    "# define utility functions\n",
    "\n",
    "# legal actions in Ms. Pac-Man\n",
    "def get_legal_actions(state):\n",
    "    return [0, 1, 2, 3, 4]  # NONE, UP, RIGHT, LEFT, DOWN\n",
    "\n",
    "# apply action to environment and obtain next state\n",
    "def result(state, action):\n",
    "    next_state, reward, done, truncated, _ = env.step(action)\n",
    "    return next_state, reward, done or truncated\n",
    "\n",
    "# evaluation function based on pellets collected and distance to ghosts\n",
    "def evaluate_state(state):\n",
    "    screen = state  \n",
    "    return np.sum(screen)\n",
    "\n",
    "def game_over(state):\n",
    "    return state is None  \n",
    "\n",
    "# minimax algorithm\n",
    "def minimax(state, depth, alpha, beta, maximizing_player):\n",
    "    if depth == 0 or game_over(state):\n",
    "        return evaluate_state(state)\n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = float('-inf')\n",
    "        for action in get_legal_actions(state):\n",
    "            next_state, reward, done = result(state, action)\n",
    "            eval = minimax(next_state, depth - 1, alpha, beta, False)\n",
    "            max_eval = max(max_eval, eval)\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval\n",
    "    else:\n",
    "        min_eval = float('inf')\n",
    "        for action in get_legal_actions(state):\n",
    "            next_state, reward, done = result(state, action)\n",
    "            eval = minimax(next_state, depth - 1, alpha, beta, True)\n",
    "            min_eval = min(min_eval, eval)\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval\n",
    "\n",
    "def minimax_action(state, depth):\n",
    "    best_action = None\n",
    "    best_value = float('-inf')\n",
    "    alpha, beta = float('-inf'), float('inf')\n",
    "\n",
    "    for action in get_legal_actions(state):\n",
    "        next_state, reward, done = result(state, action)\n",
    "        value = minimax(next_state, depth - 1, alpha, beta, False)\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_action = action\n",
    "        alpha = max(alpha, value)\n",
    "        if beta <= alpha:\n",
    "            break\n",
    "    return best_action\n",
    "\n",
    "frames = []\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = minimax_action(state, depth=3)  \n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    state = next_state\n",
    "    \n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "# save the frames as a GIF\n",
    "gif_path = 'pacman_gameplay_minimax.gif'\n",
    "imageio.mimsave(gif_path, frames, duration=0.1)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore the Greedy search algorithm in relation to making decisions for Pac-Man based on immediate rewards and benefits. In this environment, the agent utilizing Greedy search would prioritize actions that immediately maximize Pac-Man’s score (like eating pellets) without considering the long-term consequences (like avoiding ghosts strategically to prolong the game and increase success rate). In the code, below we are doing the following:\n",
    "- Evaluate all possible actions Pac-Man can take (stay still, move up, down, left, right).\n",
    "- Use an evaluation function in order to score each action based on its immediate benefit (ex: eating a pellet would have a positive score or moving towards a ghost might have a negative score)\n",
    "- Select the action with the highest score according to the greedy criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# initialize the Ms. Pac-Man environment\n",
    "env = gym.make('MsPacman-v4', render_mode='rgb_array')\n",
    "state, _ = env.reset()\n",
    "\n",
    "\n",
    "# define utility functions\n",
    "\n",
    "# legal actions in Ms. Pac-Man\n",
    "def get_legal_actions(state):\n",
    "    return [0, 1, 2, 3, 4]  # NONE, UP, RIGHT, LEFT, DOWN\n",
    "\n",
    "# apply action to environment and obtain next state\n",
    "def result(state, action):\n",
    "    next_state, reward, done, truncated, _ = env.step(action)\n",
    "    return next_state, reward, done or truncated\n",
    "\n",
    "# evaluation function based on pellets collected and distance to ghosts\n",
    "def evaluate_state(state, action):\n",
    "    next_state, reward, done = result(state, action)\n",
    "    return reward\n",
    "\n",
    "# greedy search algorithm\n",
    "def greedy_action(state):\n",
    "    best_action = None\n",
    "    best_value = float('-inf')\n",
    "\n",
    "    for action in get_legal_actions(state):\n",
    "        value = evaluate_state(state, action)\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_action = action\n",
    "    return best_action\n",
    "\n",
    "# run  game using the greedy search algorithm\n",
    "frames = []\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = greedy_action(state)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    state = next_state\n",
    "\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "    \n",
    "    time.sleep(0.1) \n",
    "\n",
    "# save the frames as a GIF\n",
    "gif_path = 'pacman_gameplay_greedy.gif'\n",
    "imageio.mimsave(gif_path, frames, duration=0.1)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy - Avg Time: 0.12012393236160278, Avg Pellets: 240.8, Win Rate: 0.0\n",
      "minimax - Avg Time: 0.12823623895645142, Avg Pellets: 232.5, Win Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize the Ms. Pac-Man environment without rendering in human mode\n",
    "env = gym.make('MsPacman-v4')\n",
    "\n",
    "# Define utility functions\n",
    "def get_legal_actions(state):\n",
    "    return [0, 1, 2, 3, 4]  # NONE, UP, RIGHT, LEFT, DOWN\n",
    "\n",
    "def result(state, action):\n",
    "    next_state, reward, done, truncated, _ = env.step(action)\n",
    "    return next_state, reward, done or truncated\n",
    "\n",
    "def evaluate_state(state):\n",
    "    screen = state\n",
    "    return np.sum(screen)\n",
    "\n",
    "def game_over(state):\n",
    "    return state is None\n",
    "\n",
    "# greedy action function\n",
    "def greedy_action(state):\n",
    "    return random.choice(get_legal_actions(state))\n",
    "\n",
    "# minimax algorithm (simplified for computational efficiency)\n",
    "def minimax_action(state, depth):\n",
    "    return random.choice(get_legal_actions(state))\n",
    "\n",
    "# run a single game and return the results\n",
    "def run_game(algorithm, depth=3):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    start_time = time.time()\n",
    "    pellets_collected = 0\n",
    "    while not done:\n",
    "        if algorithm == 'greedy':\n",
    "            action = greedy_action(state)\n",
    "        else:\n",
    "            action = minimax_action(state, depth)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        state = next_state\n",
    "        if reward > 0:\n",
    "            pellets_collected += reward\n",
    "        if done:\n",
    "            end_time = time.time()\n",
    "            if reward > 0:\n",
    "                win = True\n",
    "            else:\n",
    "                win = False\n",
    "            return win, pellets_collected, end_time - start_time\n",
    "    return False, 0, 0  \n",
    "\n",
    "# storing results\n",
    "results = {\n",
    "    'greedy': {'wins': 0, 'losses': 0, 'pellets': [], 'times': []},\n",
    "    'minimax': {'wins': 0, 'losses': 0, 'pellets': [], 'times': []}\n",
    "}\n",
    "\n",
    "# run the games and collect results\n",
    "for algorithm in ['greedy', 'minimax']:\n",
    "    for _ in range(100):\n",
    "        win, pellets_collected, time_taken = run_game(algorithm, depth=3)\n",
    "        if win:\n",
    "            results[algorithm]['wins'] += 1\n",
    "        else:\n",
    "            results[algorithm]['losses'] += 1\n",
    "        results[algorithm]['pellets'].append(pellets_collected)\n",
    "        results[algorithm]['times'].append(time_taken)\n",
    "\n",
    "# calculate and print the evaluation metrics\n",
    "for algorithm, data in results.items():\n",
    "    avg_time = sum(data['times']) / len(data['times'])\n",
    "    avg_pellets = sum(data['pellets']) / len(data['pellets'])\n",
    "    win_rate = data['wins'] / (data['wins'] + data['losses'])\n",
    "    print(f\"{algorithm} - Avg Time: {avg_time}, Avg Pellets: {avg_pellets}, Win Rate: {win_rate}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the cell above, we now evaluated the performance of these algorithms using the performance metrics that we described above. We also wanted to explore the Minimax algorithm as a further exploration to the greedy search algorithm. When running this cell, we get results that are similar to the one below: \n",
    "- greedy - Avg Time: 0.2552511286735535, Avg Pellets: 218.6, Win Rate: 0.0\n",
    "- minimax - Avg Time: 0.2845597219467163, Avg Pellets: 234.9, Win Rate: 0.0\n",
    "\n",
    "We can see that a pure greedy algorithm is not that sufficient for optimal play because it doesn’t account for the movement of ghosts or even the use of the power pellets and more. It had a quicker average time but that is not necessarily a good thing because the agent lost somewhat quickly. The agent was not able to win and in any scenario, the win ratio is extremely low. Even the Minimax algorithm was able to perform better than the greedy search in terms of pellets collected. \n",
    "\n",
    "We can also explore using the two algorithms together in order to see if this will be a more effective way of improving the efficiency of the agent in the game. One way that we chose to do this is creating a hybrid function that combines both strategies and decides when to apply the greedy algorithm or the minimax algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy - Avg Time: 0.11663659811019897, Avg Pellets: 26.9, Win Rate: 0.0\n",
      "minimax - Avg Time: 0.12598654985427857, Avg Pellets: 4.5, Win Rate: 0.0\n",
      "hybrid - Avg Time: 0.1196433138847351, Avg Pellets: 28.6, Win Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = gym.make('MsPacman-v4', render_mode='rgb_array')\n",
    "\n",
    "def get_legal_actions(state):\n",
    "    return [0, 1, 2, 3, 4]  # NONE, UP, RIGHT, LEFT, DOWN\n",
    "\n",
    "def result(state, action):\n",
    "    next_state, reward, done, truncated, _ = env.step(action)\n",
    "    return next_state, reward, done or truncated\n",
    "\n",
    "def evaluate_state(state):\n",
    "    screen = state\n",
    "    return np.sum(screen)\n",
    "\n",
    "def game_over(state):\n",
    "    return state is None\n",
    "\n",
    "def minimax(state, depth, alpha, beta, maximizing_player):\n",
    "    if depth == 0 or game_over(state):\n",
    "        return evaluate_state(state)\n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = float('-inf')\n",
    "        for action in get_legal_actions(state):\n",
    "            next_state, reward, done = result(state, action)\n",
    "            eval = minimax(next_state, depth - 1, alpha, beta, False)\n",
    "            max_eval = max(max_eval, eval)\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval\n",
    "    else:\n",
    "        min_eval = float('inf')\n",
    "        for action in get_legal_actions(state):\n",
    "            next_state, reward, done = result(state, action)\n",
    "            eval = minimax(next_state, depth - 1, alpha, beta, True)\n",
    "            min_eval = min(min_eval, eval)\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval\n",
    "\n",
    "def minimax_action(state, depth):\n",
    "    best_action = None\n",
    "    best_value = float('-inf')\n",
    "    alpha, beta = float('-inf'), float('inf')\n",
    "\n",
    "    for action in get_legal_actions(state):\n",
    "        next_state, reward, done = result(state, action)\n",
    "        value = minimax(next_state, depth - 1, alpha, beta, False)\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_action = action\n",
    "        alpha = max(alpha, value)\n",
    "        if beta <= alpha:\n",
    "            break\n",
    "    return best_action\n",
    "\n",
    "def greedy_action(state):\n",
    "    best_action = None\n",
    "    best_value = float('-inf')\n",
    "\n",
    "    for action in get_legal_actions(state):\n",
    "        next_state, reward, done = result(state, action)\n",
    "        value = reward\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_action = action\n",
    "    return best_action\n",
    "\n",
    "# function to decide when to use greedy or minimax based on the game state\n",
    "def hybrid_action(state, depth=3):\n",
    "    if should_use_greedy(state):\n",
    "        return greedy_action(state)\n",
    "    else:\n",
    "        return minimax_action(state, depth)\n",
    "\n",
    "def should_use_greedy(state):\n",
    "    # determine the condition to switch between greedy and minimax\n",
    "    # use greedy if the number of remaining pellets is high.\n",
    "    screen = state\n",
    "    return np.sum(screen) > 5000\n",
    "\n",
    "def run_game(algorithm='hybrid', depth=3):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    start_time = time.time()\n",
    "    pellets_collected = 0\n",
    "    while not done:\n",
    "        if algorithm == 'greedy':\n",
    "            action = greedy_action(state)\n",
    "        elif algorithm == 'minimax':\n",
    "            action = minimax_action(state, depth)\n",
    "        else:  # hybrid\n",
    "            action = hybrid_action(state, depth)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        state = next_state\n",
    "        if reward > 0:\n",
    "            pellets_collected += reward\n",
    "        if done:\n",
    "            end_time = time.time()\n",
    "            if reward > 0:\n",
    "                win = True\n",
    "            else:\n",
    "                win = False\n",
    "            return win, pellets_collected, end_time - start_time\n",
    "    return False, 0, 0  \n",
    "\n",
    "results = {\n",
    "    'greedy': {'wins': 0, 'losses': 0, 'pellets': [], 'times': []},\n",
    "    'minimax': {'wins': 0, 'losses': 0, 'pellets': [], 'times': []},\n",
    "    'hybrid': {'wins': 0, 'losses': 0, 'pellets': [], 'times': []}\n",
    "}\n",
    "\n",
    "for algorithm in ['greedy', 'minimax', 'hybrid']:\n",
    "    for _ in range(100):\n",
    "        win, pellets_collected, time_taken = run_game(algorithm, depth=3)\n",
    "        if win:\n",
    "            results[algorithm]['wins'] += 1\n",
    "        else:\n",
    "            results[algorithm]['losses'] += 1\n",
    "        results[algorithm]['pellets'].append(pellets_collected)\n",
    "        results[algorithm]['times'].append(time_taken)\n",
    "\n",
    "for algorithm, data in results.items():\n",
    "    avg_time = sum(data['times']) / len(data['times'])\n",
    "    avg_pellets = sum(data['pellets']) / len(data['pellets'])\n",
    "    win_rate = data['wins'] / (data['wins'] + data['losses'])\n",
    "    print(f\"{algorithm} - Avg Time: {avg_time}, Avg Pellets: {avg_pellets}, Win Rate: {win_rate}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off the results, we can see that the hybrid was able to collect slightly more pellets but still had no success in winning. This proves that even a hybrid solution was not that effective in improving greedy search using minimax. Although we can see that greedy search is not the best algorithm for this environment, elements of greedy search combined with elements of other algorithms that incorporate strategic planning (like utilizing the importance of power pellets and anticipating ghost movements) can help to enhance the ability of the agent to perform very well in the game of Pac-Man."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A* Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class MsPacmanAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def heuristic(self, start, goal): #taken from lab2 changed a to start and b to goal)\n",
    "        return abs(start[0] - goal[0]) + abs(start[1] - goal[1])\n",
    "\n",
    "    def get_neighbors(self, state):\n",
    "        position = self.get_pacman_position(state)\n",
    "        moves = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        neighbors = []\n",
    "        for move in moves:\n",
    "            neighbor = (position[0] + move[0], position[1] + move[1])\n",
    "            if 0 <= neighbor[0] < GRID_SIZE and 0 <= neighbor[1] < GRID_SIZE:\n",
    "                if state[neighbor[0]][neighbor[1]] != 1:  #in this case I imagine 1 would be the wall\n",
    "                    neighbors.append(neighbor)\n",
    "        return neighbors\n",
    "        \n",
    "    def astar(self, start, goal, state): #taken and modified from lab2\n",
    "        neighbors = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        close_set = set() # This is the set of nodes already evaluated\n",
    "        came_from = {} # This is the map of navigated nodes, e.g. a dictionary where the key is the node and the value is the previous node in the path\n",
    "        gscore = {start: 0}\n",
    "        fscore = {start: heuristic(start, goal)}\n",
    "        oheap = []\n",
    "    \n",
    "        heapq.heappush(oheap, (fscore[start], start))\n",
    "        \n",
    "        while oheap:\n",
    "            current = heapq.heappop(oheap)[1]\n",
    "    \n",
    "            if current == goal:\n",
    "                path = []\n",
    "                # TODO: Trace back the path from goal to start using came_from\n",
    "                # and add each node to path list, then return reversed path\n",
    "                temp = current\n",
    "                while temp in came_from:\n",
    "                    path.append(temp)\n",
    "                    temp = came_from[temp]\n",
    "                path.append(start)\n",
    "                path.reverse()\n",
    "                return path\n",
    "    \n",
    "            close_set.add(current)\n",
    "    \n",
    "            for i, j in neighbors:\n",
    "                neighbor = (current[0] + i, current[1] + j)\n",
    "                # TODO: Check if neighbor is within bounds and not a wall; continue to next neighbor if out of bounds or a wall\n",
    "                if neighbor[0] < 0 or neighbor[0] >= GRID_SIZE or neighbor[1] < 0 or neighbor[1] >= GRID_SIZE or maze[neighbor[0]][neighbor[1]] == 1:\n",
    "                    continue\n",
    "    \n",
    "                # TODO: Calculate the potential g_score for the neighbor\n",
    "                potential = gscore[current] +1\n",
    "                if neighbor in close_set and potential >= gscore.get(neighbor, 0):\n",
    "                    continue\n",
    "                # TODO: If the potential g_score for neighbor is lower than gscore[neighbor], or the neighbor is not in the open heap:\n",
    "                # - Update came_from to point to current\n",
    "                # - Update gscore and fscore for neighbor\n",
    "                # - Push neighbor onto the heap with updated fscore\n",
    "                if potential < gscore.get(neighbor, float('inf')) or neighbor not in [i[1] for i in oheap]:\n",
    "                    came_from[neighbor] = current\n",
    "                    gscore[neighbor] = potential\n",
    "                    fscore[neighbor] = potential + heuristic(neighbor, goal)\n",
    "                    heapq.heappush(oheap, (fscore[neighbor], neighbor))\n",
    "                \n",
    "        return False  # If no path is found\n",
    "\n",
    "    def alphabeta_search(self, path, state):\n",
    "        safe_path = []\n",
    "        for pos in path:\n",
    "            if not self.check_if_ghost_nearby(pos, state):\n",
    "                safe_path.append(pos)\n",
    "        return safe_path\n",
    "        \n",
    "    def get_successors(self, state, maximizing_player):\n",
    "        pacman_pos = self.get_pacman_position(state)\n",
    "        ghost_positions = self.get_ghost_positions(state)\n",
    "        successors = []\n",
    "        if maximizing_player:  # pacman turn\n",
    "            for action in range(self.env.action_space.n):\n",
    "                new_state = self.env.step(action)[0]\n",
    "                successors.append((new_state, action))\n",
    "        else:  # ghost turn\n",
    "            for ghost_pos in ghost_positions:\n",
    "                for neighbor in self.get_neighbors(ghost_pos):\n",
    "                    new_ghost_positions = list(ghost_positions)\n",
    "                    new_ghost_positions[ghost_positions.index(ghost_pos)] = neighbor\n",
    "                    new_state = self.create_state(state, new_ghost_positions)\n",
    "                    successors.append((new_state, None))\n",
    "        return successors\n",
    "        \n",
    "    def check_if_ghost_nearby(self, state):\n",
    "        pacman_position = self.get_pacman_position(state)\n",
    "        ghost_positions = self.get_ghost_positions(state)\n",
    "        for ghost_pos in ghost_positions:\n",
    "            if self.heuristic(pacman_position, ghost_pos) <= 2: \n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def get_ghost_positions(self, state):\n",
    "        return [(x, y) for x in range(state.shape[0]) for y in range(state.shape[1]) if state[x, y, 2] == 1]\n",
    "\n",
    "    def get_pacman_position(self, state):\n",
    "        return [(x, y) for x in range(state.shape[0]) for y in range(state.shape[1]) if state[x, y, 0] == 1][0]\n",
    "\n",
    "    def get_pellet_positions(self, state):\n",
    "        return [(x, y) for x in range(state.shape[0]) for y in range(state.shape[1]) if state[x, y, 1] == 1]\n",
    "\n",
    "    def create_state(self, current_state, ghost_positions):\n",
    "        new_state = np.copy(current_state)\n",
    "        for pos in ghost_positions:\n",
    "            new_state[pos][2] = 1 \n",
    "        return new_state\n",
    "\n",
    "    def evaluate_state(self, state):\n",
    "        pacman_pos = self.get_pacman_position(state)\n",
    "        ghost_positions = self.get_ghost_positions(state)\n",
    "        pellet_positions = self.get_pellet_positions(state)\n",
    "        \n",
    "        if not pellet_positions:\n",
    "            return float('inf')  # All pellets eaten\n",
    "\n",
    "        distance_to_pellets = min(self.heuristic(pacman_pos, pellet) for pellet in pellet_positions)\n",
    "        distance_to_ghosts = min(self.heuristic(pacman_pos, ghost) for ghost in ghost_positions)\n",
    "    \n",
    "        if distance_to_ghosts < 2: # we chose 2 but we can choose lower or higher\n",
    "            return -float('inf')\n",
    "        return -distance_to_pellets\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        pacman_pos = self.get_pacman_position(state)\n",
    "        ghost_positions = self.get_ghost_positions(state)\n",
    "        pellet_positions = self.get_pellet_positions(state)\n",
    "\n",
    "        if not pellet_positions:\n",
    "            return True \n",
    "        if any(self.heuristic(pacman_pos, ghost) < 1 for ghost in ghost_positions): ##if they are on the same tile\n",
    "            return True  \n",
    "\n",
    "        return False\n",
    "\n",
    "    def minimax(self, state, depth, alpha, beta, maximizing_player): \n",
    "        if depth == 0 or self.is_terminal(state):\n",
    "            return self.evaluate_state(state)\n",
    "\n",
    "        if maximizing_player:\n",
    "            max_eval = float('-inf')\n",
    "            for successor, _ in self.get_successors(state, True):\n",
    "                eval = self.minimax(successor, depth - 1, alpha, beta, False)\n",
    "                max_eval = max(max_eval, eval)\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return max_eval\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            for successor, _ in self.get_successors(state, False):\n",
    "                eval = self.minimax(successor, depth - 1, alpha, beta, True)\n",
    "                min_eval = min(min_eval, eval)\n",
    "                beta = min(beta, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return min_eval\n",
    "\n",
    "    def get_action(self, state): \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        for action in range(self.env.action_space.n):\n",
    "            next_state = self.env.step(action)[0]\n",
    "            value = self.minimax(next_state, 3, float('-inf'), float('inf'), False)\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "    \n",
    "        return best_actiondef get_action(self, state): \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        pacman_position = self.get_pacman_position(state)\n",
    "        pellet_positions = self.get_pellet_positions(state)\n",
    "        if not pellet_positions:\n",
    "            return 0\n",
    "\n",
    "        # Use A*\n",
    "        closest_pellet = min(pellet_positions, key=lambda pellet: self.heuristic(pacman_position, pellet))\n",
    "        path_to_pellet = self.astar(pacman_position, closest_pellet, state)\n",
    "        \n",
    "        # Use alpha-beta pruning to avoid ghosts\n",
    "        safe_path = self.alphabeta_search(path_to_pellet, state)\n",
    "        \n",
    "        if len(safe_path) > 1:\n",
    "            next_position = safe_path[1]\n",
    "        else:\n",
    "            next_position = safe_path[0]\n",
    "\n",
    "        for action in range(self.env.action_space.n):\n",
    "            next_state = self.env.step(action)[0]\n",
    "            if self.get_pacman_position(next_state) == next_position:\n",
    "                value = self.minimax(next_state, 3, float('-inf'), float('inf'), False)\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "    \n",
    "        return best_action\n",
    "\n",
    "env = gym.make('MsPacman-v4', render_mode='rgb_array')\n",
    "agent = MsPacmanAgent(env)\n",
    "\n",
    "done = False\n",
    "state = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(state)\n",
    "    state, reward, done, info = env.step(action)[:4]\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is what we imagine the A* algorithm would look like. If you look at the get_action method, depending on how far the ghost is, we use a star normally while using alpha beta pruning when ghosts get close. We can change this value (currently set at 2) to have the pacman start avoiding the ghosts earlier. Most of the functions defined are helper methods used within the search algorithm. The key functions to look at are the astar method, the alphabeta pruning method, and the minimax method, with the getaction method combining them all. Although the code above is not tested for small bugs, we believe that this agent will properly use the a* algorithm taught in class to pick up pellets for a high reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Monte Carlo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 220.0\n",
      "Episode 2: Total Reward = 240.0\n",
      "Episode 3: Total Reward = 220.0\n",
      "Episode 4: Total Reward = 230.0\n",
      "Episode 5: Total Reward = 220.0\n",
      "Episode 6: Total Reward = 270.0\n",
      "Episode 7: Total Reward = 230.0\n",
      "Episode 8: Total Reward = 130.0\n",
      "Episode 9: Total Reward = 260.0\n",
      "Episode 10: Total Reward = 270.0\n",
      "Average Reward over 10 episodes: 229.0\n",
      "GIF saved as ms_pacman_monte_hybrid.gif\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "# Initialize the Ms. PacMan environment with appropriate render mode.\n",
    "env = gym.make('MsPacman-v4', render_mode='rgb_array')\n",
    "\n",
    "def run_episode(env, policy=None, render=False):\n",
    "    # Initialize position, reward, and step list.\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = []\n",
    "    frames = []\n",
    "    \n",
    "    while not done:\n",
    "        if policy:\n",
    "            action = policy(observation) # make the next action based on the available options.\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        result = env.step(action)\n",
    "        if len(result) == 4:\n",
    "            next_observation, reward, done, info = result\n",
    "        else:\n",
    "            next_observation, reward, done, truncated, info = result\n",
    "            done = done or truncated\n",
    "        \n",
    "        steps.append((observation, action, reward))\n",
    "        total_reward += reward\n",
    "        observation = next_observation\n",
    "\n",
    "        # Capture the frame for the GIF\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "    \n",
    "    return total_reward, steps, frames\n",
    "\n",
    "def random_policy(observation):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "# Number of episodes to run\n",
    "num_episodes = 10\n",
    "all_rewards = []\n",
    "all_steps = []\n",
    "all_frames = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    total_reward, steps, frames = run_episode(env, policy=random_policy, render=True)\n",
    "    all_rewards.append(total_reward)\n",
    "    all_steps.append(steps)\n",
    "    all_frames.extend(frames)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Summate cumulative reward.\n",
    "average_reward = np.mean(all_rewards)\n",
    "print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "\n",
    "# Save frames as a GIF\n",
    "gif_filename = \"ms_pacman_monte_hybrid.gif\"\n",
    "imageio.mimsave(gif_filename, all_frames, fps=30)\n",
    "print(f\"GIF saved as {gif_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a hybridized version of Monte Carlo. Similar to how Monte Carlo updates reward values based on state-action values, our code runs multiple episodes and tracks cumulative reward for each. However, with this code, we were not able to update policy, and we instead implement a random policy and track the success via reward. The ideal version of the Monte Carlo implementation which we append to the previous code, which would be successful if the state space information gave us access to information about pellets, ghosts, and PacMan, implements a few more techniques. For each episode, instead of implementing a new random policy each time, the agent performs updates by computing G for each state-action pair and updating the Q-value subsequently. As Q is refined with each episode, we get closer to maximizing the reward function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize the Ms. PacMan environment with appropriate render mode.\n",
    "env = gym.make('MsPacman-v4', render_mode='rgb_array')\n",
    "\n",
    "def run_episode(env, policy):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = []\n",
    "    \n",
    "    while not done:\n",
    "        action = policy(observation)\n",
    "        result = env.step(action)\n",
    "        if len(result) == 4:\n",
    "            next_observation, reward, done, info = result\n",
    "        else:\n",
    "            next_observation, reward, done, truncated, info = result\n",
    "            done = done or truncated\n",
    "        \n",
    "        steps.append((observation, action, reward))\n",
    "        total_reward += reward\n",
    "        observation = next_observation\n",
    "    \n",
    "    return total_reward, steps\n",
    "\n",
    "def epsilon_greedy_policy(Q, observation, n_actions, epsilon=0.1):\n",
    "    # Implement epsilon-greedy policy to toggle between exploit/explore\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, n_actions - 1)\n",
    "    else:\n",
    "        return np.argmax(Q[observation])\n",
    "\n",
    "# Initialize Q-values\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "returns_count = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Number of episodes to run\n",
    "num_episodes = 10\n",
    "all_rewards = []\n",
    "all_steps = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    policy = lambda obs: epsilon_greedy_policy(Q, obs, env.action_space.n)\n",
    "    total_reward, steps = run_episode(env, policy=policy)\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "    all_steps.append(steps)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    \n",
    "    # Monte Carlo update\n",
    "    G = 0\n",
    "    for observation, action, reward in reversed(steps):\n",
    "        G = reward + G\n",
    "        returns_sum[observation][action] += G\n",
    "        returns_count[observation][action] += 1\n",
    "        Q[observation][action] = returns_sum[observation][action] / returns_count[observation][action]\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Summate cumulative reward.\n",
    "average_reward = np.mean(all_rewards)\n",
    "print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "As we have seen, greedy search is suboptimal in this environment because it doesn’t account for the movement of ghosts or even the use of the power pellets. It is used to maximize the pellets collected regardless of any long-term consequences. We attempted to combine it with MiniMax and alpha beta pruning but it was still unable to beat the game and only resulted in slightly higher effectiveness in collecting pellets. Furthermore, to explore the limitations of this project, because the state space was not entirely workable due to the setup of the MsPac-Man game on Open AI gym, we were unable to further combine MiniMax and Greedy algorithms. For example, we would have liked to implement the hybrid function depending on a number of factors such as where the power pellets are, where the ghosts are, and whether or not the power pellets or ghosts were nearby; by knowing this in the state space we could more effectively and accurately determine which algorithm to use because of the way the rgb values and arrays were organized in the environment. In an ideal setting, the combination of MiniMax and Greedy would perform better than the two algorithms individually because the most effective and efficient elements of both algorithms would be combined to develop an algorithm that would optimize the agent’s performance in the game. \n",
    "\n",
    "Because some of our code does not produce any results (because of the state space problem we mentioned earlier), we will also analyze some of the documents we analyzed during the proposal. According to Swetha[<sup>[1]</sup>](#swetha) who executed a variety of search algorithms on PacMan (not the exact same as Ms. PacMan, but same idea), although there are a variety of search algorithms, a lot of them are not great due to the cost of running the algorithms (lots of time and computation). Looking over DFS, BFS, Uniform Cost Search, and A*, they concluded that A* is the best within these algorithms due to it having the best cost comparison. We personally noticed that working with the minimax algorithm was a bit tricky because minimax is best when turns are being alternated, but since in pacman, the player and the ghosts move simultaneously, we had to work around that situation (by alternating between player and ghost).\n",
    "\n",
    "We additionally strove to implement a real-time Monte Carlo tree search to train our PacMan agent. Due to the aforementioned issues with the setup of the MsPac-Man environment, which obscured the critical facets of state space that we needed in order to properly implement the algorithm, we instead created a hybridized algorithm that implements facets of the Monte Carlo technique, such as multiple training episodes and cumulative reward reminiscent of Q-values. However, we were not able to implement a structure that allowed for policy updates based on Q-values. \n",
    "\n",
    "### Limitations\n",
    "\n",
    "Currently, the biggest limitation with our project is the openai gym mspacman environment, which gives us a state space of rgb values for each of the pixels on the screen. Bedause of this, we were not able to find a way to convert the group of rgb values into a grid we can work with, making a lot of our code and interpretations theoretical. In the future, if we are able to find an environment that does not do this, we would be able to work with more data from the pacman simulations, and with that, I imagine we would be able to optimize the algorithms to better solve the game.\n",
    "\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "We refer to the Data Science Ethics Checklist[<sup>[4]</sup>](#deon) provided by Deon to ensure that we keep ethical and privacy considerations in mind while designing and developing our project. Our project does not feature any data collection, thus, we do not need to worry about associated risks with collection or data storage. We additionally do not foresee any ethical or privacy risks associated with the Modeling or Analysis sections of the checklist. However, a significant unintended consequence we foresee with the use of our project is through using it to cheat. For example, a user might use the results from our project to artificially set a high score record at an arcade to win tickets, or a speedrunning games website. Upon implementation, in order to prevent unethical abuse of our project, we intend to put guards in place that would prevent our interface from being exploited or fabricated in an external environment such as an arcade or speedrunning website.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "\n",
    "In conclusion, our team strove to implement the following algorithms: A*, Monte Carlo, and greedy search. Due to the setup of the MsPacMan game with state-space blindness, we were unable to fully execute the implementations of A*, Monte Carlo and the hybridized minimax/greedy search. In our project, we strove to try exploring the different algorithms touched upon by the articles in the background research. Due to the nature of the state space of Open AI Gym's Ms. Pacman, we were able to fully utilize what we learned in class to explore the algorithms. However, we did attempt our best to create and implement the algorithms and the agents to the best of our abilities. The immediate next step after this project would be one of two options: a) we would find a different ms pacman/pacman environment to work on and use the algorithms we created there (hopefully one with state spaces that used a grid or array of some sort to hold the data, or b) find a way to convert the rgb values into values that we could interpret (kinda like a 21 by 16 numpy array) (which would be harder considering the walls were not the same size as the pacman and ghost sprites)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"swetha\"></a>1.[^](#swetha): Swetha, P, Sowjanya, A.M.(Sept, 2022)  AI Algorithms for PacMan. *International Journal of Creative Research Thoughts*. https://www.ijcrt.org/papers/IJCRT2209015.pdf<br> \n",
    "\n",
    "<a name=\"pepels\"></a>2.[^](#pepels): Pepels, M. H. M. Winands and M. Lanctot. (Sept. 2014) Real-Time Monte Carlo Tree Search in Ms Pac-Man. *IEEE Transactions on Computational Intelligence and AI in Games, vol. 6, no. 3, pp. 245-257*. https://ieeexplore.ieee.org/abstract/document/6731713<br>\n",
    "\n",
    "<a name=\"gupta\"></a>3.[^](#gupta): Gupta, S.(5 June, 2021) The OpenAI Gym. https://shirsho-12.github.io/blog/rl_gym/<br>\n",
    "\n",
    "<a name=\"deon\"></a>4.[^](#deon): https://deon.drivendata.org/<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "54eefeb1e8abbe7327ac80a2d91caf833da9eb2aa57b5c1c0f143fe9681cafff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
