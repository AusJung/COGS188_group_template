{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b9a425-a33b-4d3f-bf42-a02e484d63fe",
   "metadata": {},
   "source": [
    "We tried working with the observation/state space of the Open AI gym MsPacman, but from what we could figure out, the state space was given to us based off the rgb values of each pixel. The numpy array given to us gives us rgb values for each pixel rather than the grid, with values for each coordinate, We tried to look for a way to convert the rgb values to grid values with pellet, ghost, and pacman information, but we were unable to find a way to do so. Therefore, some of our code doesnâ€™t run and our code below is mostly based off what we imagine would be correct if we were to have state spaces that were workable (we tried for days to no avail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0456187-d1f4-40c6-87c9-30da55cda746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class MsPacmanAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def heuristic(self, start, goal): #taken from lab2 changed a to start and b to goal)\n",
    "        return abs(start[0] - goal[0]) + abs(start[1] - goal[1])\n",
    "\n",
    "    def get_neighbors(self, state):\n",
    "        position = self.get_pacman_position(state)\n",
    "        moves = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        neighbors = []\n",
    "        for move in moves:\n",
    "            neighbor = (position[0] + move[0], position[1] + move[1])\n",
    "            if 0 <= neighbor[0] < GRID_SIZE and 0 <= neighbor[1] < GRID_SIZE:\n",
    "                if state[neighbor[0]][neighbor[1]] != 1:  #in this case I imagine 1 would be the wall\n",
    "                    neighbors.append(neighbor)\n",
    "        return neighbors\n",
    "\n",
    "    def get_successors(self, state, maximizing_player):\n",
    "        pacman_pos = self.get_pacman_position(state)\n",
    "        ghost_positions = self.get_ghost_positions(state)\n",
    "        successors = []\n",
    "        if maximizing_player:  # pacman turn\n",
    "            for action in range(self.env.action_space.n):\n",
    "                new_state = self.env.step(action)[0]\n",
    "                successors.append((new_state, action))\n",
    "        else:  # ghost turn\n",
    "            for ghost_pos in ghost_positions:\n",
    "                for neighbor in self.get_neighbors(ghost_pos):\n",
    "                    new_ghost_positions = list(ghost_positions)\n",
    "                    new_ghost_positions[ghost_positions.index(ghost_pos)] = neighbor\n",
    "                    new_state = self.create_state(state, new_ghost_positions)\n",
    "                    successors.append((new_state, None))\n",
    "        return successors\n",
    "\n",
    "    def get_ghost_positions(self, state):\n",
    "        return [(x, y) for x in range(state.shape[0]) for y in range(state.shape[1]) if state[x, y, 2] == 1]\n",
    "\n",
    "    def get_pacman_position(self, state):\n",
    "        return [(x, y) for x in range(state.shape[0]) for y in range(state.shape[1]) if state[x, y, 0] == 1][0]\n",
    "\n",
    "    def get_pellet_positions(self, state):\n",
    "        return [(x, y) for x in range(state.shape[0]) for y in range(state.shape[1]) if state[x, y, 1] == 1]\n",
    "\n",
    "    def create_state(self, current_state, ghost_positions):\n",
    "        new_state = np.copy(current_state)\n",
    "        for pos in ghost_positions:\n",
    "            new_state[pos][2] = 1 \n",
    "        return new_state\n",
    "\n",
    "    def evaluate_state(self, state):\n",
    "        pacman_pos = self.get_pacman_position(state)\n",
    "        ghost_positions = self.get_ghost_positions(state)\n",
    "        pellet_positions = self.get_pellet_positions(state)\n",
    "        \n",
    "        if not pellet_positions:\n",
    "            return float('inf')  # All pellets eaten\n",
    "\n",
    "        distance_to_pellets = min(self.heuristic(pacman_pos, pellet) for pellet in pellet_positions)\n",
    "        distance_to_ghosts = min(self.heuristic(pacman_pos, ghost) for ghost in ghost_positions)\n",
    "    \n",
    "        if distance_to_ghosts < 2: # we chose 2 but we can choose lower or higher\n",
    "            return -float('inf')\n",
    "        return -distance_to_pellets\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        pacman_pos = self.get_pacman_position(state)\n",
    "        ghost_positions = self.get_ghost_positions(state)\n",
    "        pellet_positions = self.get_pellet_positions(state)\n",
    "\n",
    "        if not pellet_positions:\n",
    "            return True \n",
    "        if any(self.heuristic(pacman_pos, ghost) < 1 for ghost in ghost_positions): ##if they are on the same tile\n",
    "            return True  \n",
    "\n",
    "        return False\n",
    "\n",
    "    def minimax(self, state, depth, alpha, beta, maximizing_player): \n",
    "        if depth == 0 or self.is_terminal(state):\n",
    "            return self.evaluate_state(state)\n",
    "\n",
    "        if maximizing_player:\n",
    "            max_eval = float('-inf')\n",
    "            for successor, _ in self.get_successors(state, True):\n",
    "                eval = self.minimax(successor, depth - 1, alpha, beta, False)\n",
    "                max_eval = max(max_eval, eval)\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return max_eval\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            for successor, _ in self.get_successors(state, False):\n",
    "                eval = self.minimax(successor, depth - 1, alpha, beta, True)\n",
    "                min_eval = min(min_eval, eval)\n",
    "                beta = min(beta, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return min_eval\n",
    "\n",
    "    def get_action(self, state): \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        for action in range(self.env.action_space.n):\n",
    "            next_state = self.env.step(action)[0]\n",
    "            value = self.minimax(next_state, 3, float('-inf'), float('inf'), False)\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "    \n",
    "        return best_action\n",
    "\n",
    "env = gym.make('MsPacman-v4', render_mode='rgb_array')\n",
    "agent = MsPacmanAgent(env)\n",
    "\n",
    "done = False\n",
    "state = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(state)\n",
    "    state, reward, done, info = env.step(action)[:4]\n",
    "    env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
